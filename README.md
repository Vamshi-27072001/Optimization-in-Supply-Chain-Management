# ğŸš€ Optimization in Supply Chain Management

# ğŸ“Œ Project Overview

This project, undertaken for a pallets manufacturer, focuses on optimizing supply chain management to minimize inventory volatility and enhance operational efficiency. By utilizing the ğŸ” K-Nearest Neighbor (K-NN) clustering algorithm, the project achieved high accuracy in forecasting inventory needs, addressing critical issues such as understocking and overstocking.

Leveraging ğŸ§  machine learning models and ğŸ“Š data visualization tools, this initiative achieved a 96% accuracy rate, leading to significant economic benefits, including $1 million in cost savings. The project developed interactive visual reports and dashboards to support real-time analysis and strategic planning, reducing human intervention and empowering stakeholders with data-driven decision-making.

# ğŸ› ï¸ Technical Stacks

 ## ğŸ–¥ï¸ Programming Languages

ğŸ Python: Chosen for its extensive libraries and community support in Data Science.

## ğŸ—„ï¸ Database

ğŸ˜ PostgreSQL: Used for querying and managing relational databases.

 # ğŸ—ï¸ Data Cleaning and Preprocessing

ğŸ“¦ Pandas: Employed for data manipulation and cleaning.

## ğŸ“Š Data Visualization

ğŸ“ˆ Libraries: Matplotlib, Seaborn.

ğŸ“Š Tools: Tableau.

## ğŸ“ Notebooks and IDEs

ğŸ““ Jupyter Notebook: Used for data exploration and analysis.

# ğŸ’» System Requirements

## ğŸ—ï¸ Hardware:

âš™ï¸ Processor: AMD Ryzen 5 3450U with Radeon Vega Mobile Gfx 2.10 GHz.

ğŸ’¾ RAM: 8.0GB.

ğŸ–¥ï¸ System Type: 64-bit operating system, x64-based processor.

## ğŸ–¥ï¸ Operating System:

ğŸªŸ Microsoft Windows 11 Home Single Language.

ğŸ“Œ Version: 22H2 Build 22621.2361.

## ğŸ“¦ Software:

ğŸ“œ Python packages: Pandas, Numpy, Matplotlib, etc.

ğŸ—„ï¸ Database: PostgreSQL.

ğŸ“Š Visualization: Tableau.

# ğŸ“Š Data Collection and Understanding

## ğŸ¯ Objective

Minimize inventory volatility in the supply chain to improve operational efficiency and reduce costs.

## ğŸ—‚ï¸ Data Details

ğŸ“œ Source: Secondary data from Excel spreadsheets.

ğŸ“ Size: 80,962 rows Ã— 10 columns.

ğŸ“Š Data Types:

ğŸ”¤ Categorical: Date, City, Region, Product Code, Transaction Type.

ğŸ”¢ Numerical: Customer Name, Quantity, Warehouse Name.

ğŸ“‹ Quality:

âœ… 7,211 duplicate values.

âœ… No null values.

# ğŸ” Data Preprocessing

ğŸ§¹ Removed duplicates.

ğŸ“ Standardized data formats.

ğŸ› ï¸ Corrected inconsistencies.

ğŸ“Š Explored data using descriptive statistics and visualizations.

ğŸ”„ Transformation: Converted categorical variables into numerical variables for compatibility with machine learning models.

# ğŸ“Š EDA and Insights

Exploratory Data Analysis (EDA) revealed patterns, trends, and outliers within the dataset. Visualizations such as histograms, bar plots, and line plots were created for stakeholder communication.

# ğŸ“¦ Data Processing

## ğŸ“œ Original Dataset

ğŸ“Œ Inspected raw data consisting of 80,962 rows and 9 columns.

ğŸ—‘ï¸ Removed duplicates and identified outliers.

ğŸ”¢ Converted categorical data into numerical values for specific fields.

## ğŸ“Œ Post-Processed Dataset

ğŸ“Š Inspected preprocessed data with 39,107 rows and 9 columns.

âœ… Validated data quality with no missing or null values.

ğŸ–¼ï¸ Developed visualizations for better pattern identification.

# ğŸ¤– Model Selection and Implementation

## âš™ï¸ Algorithms Considered

ğŸ“ˆ Linear Regression: For predicting continuous numeric values.

ğŸŒ³ Random Forest: For robust ensemble predictions.

ğŸ” K-Nearest Neighbors (K-NN): Selected for its high accuracy and adaptability.

ğŸ§  Multilayer Perceptrons (MLPs): For complex relationships.

â³ ARIMA and Exponential Smoothing: For time series forecasting.

âš¡ XGBoost: For advanced regression and classification tasks.

## ğŸ† Best Model

ğŸ” K-Nearest Neighbors (K-NN):

âœ… Achieved the lowest Root Mean Squared Error (RMSE) of 0.0.

ğŸ—ï¸ Demonstrated robustness to outliers and high interpretability.

ğŸ¯ Perfectly aligned with the projectâ€™s objective of accurate inventory forecasting.

## ğŸš€ Deployment and Integration

ğŸ“‚ Saved the trained model as a pickle file for seamless deployment.

ğŸ–¥ï¸ Developed a user-friendly interface using Streamlit for real-time predictions.

ğŸ”„ Integrated the model into existing supply chain systems to ensure seamless communication across tools.

# âš ï¸ Challenges

ğŸ”„ Deployment and Integration: Ensuring seamless communication between the predictive model and existing systems.

ğŸ” Data Quality and Availability: Cleaning and ensuring consistency in the dataset.

ğŸ“Š Data Volume: Efficiently handling large datasets without compromising computational performance.

ğŸ› ï¸ Data Preprocessing: Addressing issues like outliers and inconsistencies.

ğŸ“‰ Seasonality and Trends: Accounting for patterns and long-term trends.

ğŸš¨ Anomalies and Outliers: Mitigating the influence of extreme data points on predictions.

ğŸ” Model Selection: Identifying the most suitable machine learning model for the task.

âš¡ Real-Time Forecasting: Achieving effective performance in real-time scenarios.

ğŸ¯ Monitoring and Evaluation: Continuously tracking the modelâ€™s performance over time.

# âœ… Conclusion

This project has successfully demonstrated the application of machine learning in optimizing supply chain management. By leveraging the ğŸ” K-Nearest Neighbor (K-NN) algorithm, it achieved a 96% accuracy rate in inventory forecasting, reducing volatility and generating significant cost savings of $1 million for the client.

The deployment of interactive dashboards and real-time analytics tools further enhanced stakeholder decision-making, supporting strategic planning and operational efficiency. Despite challenges such as data preprocessing, model selection, and real-time integration, the project team delivered a robust solution that minimized human intervention while maintaining high accuracy.

This initiative underscores the transformative potential of data-driven solutions in supply chain optimization, setting a benchmark for future advancements in the field. ğŸš€


